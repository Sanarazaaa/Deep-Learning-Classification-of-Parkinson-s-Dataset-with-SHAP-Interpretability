{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYRGYfJbSGx1a26Fg35/xX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanarazaaa/Deep-Learning-Classification-of-Parkinson-s-Dataset-with-SHAP-Interpretability/blob/main/Deep_Learning_Classification_of_Parkinson%E2%80%99s_Dataset_with_SHAP_Interpretability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorFlow if needed\n",
        "# !pip install tensorflow\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Upload CSV\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Step 2: Load CSV\n",
        "data = pd.read_csv(file_name)\n",
        "\n",
        "# Step 3: Drop empty columns\n",
        "data = data.drop(columns=[col for col in data.columns if data[col].isnull().sum() == len(data)])\n",
        "\n",
        "# Step 4: Select target column (multi-class)\n",
        "target_col = 'Replication'  # replace with your target column if different\n",
        "y = data[target_col]\n",
        "\n",
        "# Encode target if categorical\n",
        "if y.dtype == 'object':\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y)\n",
        "\n",
        "# Step 5: Select features\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
        "if target_col in categorical_cols:\n",
        "    categorical_cols.remove(target_col)\n",
        "\n",
        "X_cat = pd.get_dummies(data[categorical_cols])       # One-hot encode categorical\n",
        "X_num = data.select_dtypes(include=['int64', 'float64'])  # Numeric columns\n",
        "\n",
        "# Combine numeric and one-hot encoded features\n",
        "X_processed = pd.concat([X_num, X_cat], axis=1)\n",
        "\n",
        "# Store feature names before converting to numpy array\n",
        "feature_names = X_processed.columns.tolist()\n",
        "\n",
        "X = X_processed.values\n",
        "\n",
        "# Ensure numeric type\n",
        "X = X.astype('float32')\n",
        "y = y.astype('int')\n",
        "\n",
        "# Check shapes\n",
        "print(\"Feature shape:\", X.shape)\n",
        "print(\"Target shape:\", y.shape)\n",
        "print(\"Target unique values:\", np.unique(y))\n",
        "\n",
        "# Step 6: Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 7: Build multi-class neural network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(X.shape[1],)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(len(np.unique(y)), activation='softmax'))  # number of classes\n",
        "\n",
        "# Step 8: Compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 9: Train model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2)\n",
        "\n",
        "# Step 10: Plot training & validation accuracy/loss\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 11: Evaluate on test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Step 12: Predict on test set\n",
        "predictions = model.predict(X_test)\n",
        "predicted_classes = np.argmax(predictions, axis=1)  # get class with highest probability\n",
        "\n",
        "# Step 13: Confusion matrix & classification report\n",
        "cm = confusion_matrix(y_test, predicted_classes)\n",
        "cr = classification_report(y_test, predicted_classes)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "\n",
        "# Step 14: Optional - first 10 predictions vs true labels\n",
        "print(\"First 10 Predictions:\", predicted_classes[:10])\n",
        "print(\"First 10 True labels:\", y_test[:10])"
      ],
      "metadata": {
        "id": "zebHa2oIjpg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Binarize labels\n",
        "y_test_bin = label_binarize(y_test, classes=np.unique(y))\n",
        "pred_prob = model.predict(X_test)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "for i in range(y_test_bin.shape[1]):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], pred_prob[:, i])\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {auc(fpr,tpr):.2f})')\n",
        "\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Multi-class ROC Curves')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wvRkr4QtsdXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "X_embedded = TSNE(n_components=2, random_state=42).fit_transform(X)\n",
        "plt.figure(figsize=(8,6))\n",
        "for class_val in np.unique(y):\n",
        "    idx = np.where(y==class_val)\n",
        "    plt.scatter(X_embedded[idx,0], X_embedded[idx,1], label=f'Class {class_val}')\n",
        "plt.legend()\n",
        "plt.title('t-SNE Projection of Parkinson\\'s Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HC4Vesa-ssbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "for i, v in enumerate(importances):\n",
        "    print(f\"{feature_names[i]}: {v:.4f}\")\n",
        "\n",
        "# Optional: plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.barh(feature_names, importances)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(\"Random Forest Feature Importance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C6i0z8bVtBsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.KernelExplainer(model.predict, X_train[:50])  # smaller sample for speed\n",
        "shap_values = explainer.shap_values(X_test[:20])\n",
        "\n",
        "shap.summary_plot(shap_values, X_test[:20], feature_names=feature_names)\n"
      ],
      "metadata": {
        "id": "PhObQZoMtSNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=16, callbacks=[early_stop])\n"
      ],
      "metadata": {
        "id": "Kv99MJoutW4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# After one-hot encoding your labels\n",
        "from sklearn.preprocessing import label_binarize\n",
        "y_test_bin = label_binarize(y_test, classes=np.unique(y))\n",
        "y_pred_bin = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "psU9DRT8tZTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "# Load the current notebook\n",
        "notebook_filename = '/content/your_notebook.ipynb'\n",
        "nb = nbformat.read(notebook_filename, as_version=5)\n",
        "\n",
        "# Remove broken widget metadata\n",
        "if 'widgets' in nb['metadata']:\n",
        "    nb['metadata'].pop('widgets')\n",
        "\n",
        "# Save cleaned notebook\n",
        "nbformat.write(nb, notebook_filename)\n",
        "print(\"Cleaned notebook saved.\")\n"
      ],
      "metadata": {
        "id": "4w0bSQhw20ra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}